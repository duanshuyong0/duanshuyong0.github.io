

### PAPER-URL

https://arxiv.org/pdf/1611.09940.pdf

### 基于深度强化学习的组合优化问题

本文提出了一个利用神经网络和强化学习解决组合优化问题的框架。我们主要研究旅行商问题（TSP），并训练一个递归神经网络，在给定一组城市坐标的情况下，预测不同城市排列的分布。以负游程长度作为奖励信号，采用策略梯度法对电流神经网络的参数进行优化。我们将在一组训练图上学习网络参数与在个别测试图上学习网络参数进行比较。尽管计算量大，但在缺乏工程和启发式设计的情况下，神经组合优化在最多100个节点的二维欧几里德图上获得了接近最优的结果。同样的方法也适用于背包，这是另一个NP难题，它可以为200个物品的情况得到最优解。

**1）简介**

组合优化是计算机科学中的一个基本问题。一个典型的例子是旅行商问题（TSP），在给出一个图的情况下，需要搜索排列空间，以找到具有最小总边权（旅行长度）的节点的最佳序列。TSP及其变体在规划、制造、遗传学等领域有着无数的应用（参见（Applegate等人，2011年）的概述）。

即使在二维欧几里得情况下（Papadimitriou，1977），找到最佳TSP解也是NP困难的，其中节点是二维点，边缘权重是点对之间的欧几里得差。在实践中，TSP求解者依靠手工设计的启发式方法来指导他们的搜索过程，从而有效地找到有竞争力的旅游路线。尽管这些启发式方法在TSP上很好地工作，但是一旦问题语句发生轻微变化，就需要对它们进行修改。相比之下，机器学习方法有潜力通过自动发现基于训练数据的自己的启发式方法来应用于许多优化任务，因此比仅针对一个任务优化的求解器需要更少的手工工程。

虽然大多数成功的机器学习技术都属于监督学习家族，即学习从训练输入到输出的映射，但监督学习并不适用于大多数组合优化问题，因为人们无法获得最优标签。但是，可以使用验证器比较一组解决方案的质量，并向学习算法提供一些奖励反馈。因此，我们采用强化学习（RL）范式来处理组合优化问题。我们从经验上证明，即使使用最优解作为标记数据来优化监督映射，与探索不同旅行并观察其相应回报的RL代理相比，其泛化效果也相当差。

我们提出了一种利用增强学习和神经网络解决组合优化问题的框架——神经组合优化。我们考虑两种基于政策梯度的方法（Williams，1992）。第一种方法被称为RL预训练，它使用训练集来优化循环神经网络（RNN），该网络以期望的回报为目标，参数化随机策略而非解。在测试时，策略是固定的，通过贪婪的解码或采样进行推理。第二种方法称为主动搜索，不涉及预培训。

它从一个随机策略开始，在一个测试实例上迭代优化RNN参数，再次使用预期的奖励目标，同时跟踪在搜索过程中抽样的最佳解决方案。我们发现，在实践中，将RL预训练与主动搜索相结合效果最好。

在具有多达100个节点的二维欧几里德图上，神经组合优化显著优于TSP的监督学习方法（Vinyals等人，2015b），并在允许更多计算时间时获得接近最优的结果。我们通过对背包问题的相同方法的测试来说明它的灵活性，对于最多有200个物品的情况，我们得到了最佳的结果。这些结果使我们深入了解了如何将神经网络作为解决组合优化问题的通用工具，尤其是那些难以设计启发式算法的问题。

**2）前置工作**

