智能聊天机器人的发展经历了 3 代不同的技术。 

第一代是基于特征工程。有大量的逻辑判断，如ifthen;elsethen。 

第二代是基于检索库。给定一个问题或者聊天，从检索库中找到与已有答案最匹配的答案。 

第三代是基于深度学习。采用 seq2seq+Attention 模型，经过大量的训练，根据输入生成相应的输出。

下面我们就来看看基于深度学习的聊天机器人的 seq2seq+Attention 模型原理和构建方法。

seq2seq 模型是一个翻译模型，主要是把一个序列翻译成另一个序列。它的基本思想是用两个 RNNLM，一个作为编码器，另一个作为解码器，组成 RNN 编码器-解码器。

这是一种适合处理由一个上下文(context)生成一个目标(target)的通用处理模型。因此， 对于一个句子对<X, Y>，当输入给定的句子 X，通过编码器-解码器框架来生成目标句子 Y。X 和 Y 可以是不同语言，这就是机器翻译;X 和 Y 可以是对话的问句和答句，这就是聊天机器人; X 和 Y 可以是图片和这个图片的对应描述(这就是第 12 章要讲的看图说话)。 

![](https://ws4.sinaimg.cn/large/006tKfTcly1g0z96derqnj313c0b8t9p.jpg)



X 由 x1、x2 等的词序列组成，Y 也由 y1、y2 等的词序列组成。编码器对输入的 X 进行编码， 生成中间语广编码 C，然后解码器对中间语广编码 C 进行解码，在每个 i 时刻，结合已经生成 的 y1, y2,..., yi-1 的历史信息生成 Yi。但是，这个框架有一个缺点，就是生成的句子中每一个词采 用的中间语广编码是相同的，都是 C。因此，在句子比较短的时候，还能比较贴切，句子长时， 就明显不合语广了。 

在实际实现聊天系统的时候，一般编码器和解码器都采用 RNN 模型以及 RNN 模型的改进 模型 LSTM。当句子长度超过 30 以后，LSTM 模型的效果会急剧下降，一般此时会引入 Attention 模型，对长句子来说能够明显提升系统效果。 

Attention 机制是认知心理学层面的一个的的，它是的当人在做一件事情的时候，会专注地 做这件事而忽略周围的其他事。例如，人在专注地看这本本，会忽略旁边人说话的声音。这种 机制应用在聊天机器人、机器翻译等领域，就把源句子中对生成句子重要的关键词的权重提高， 产生出更准确的应答。 

增加了 Attention 模型的编码器-解码器框架如图 11-3 所示。
现在的中间语广编码变成了不断变化的 Ci，能够生产更准确的目标 Yi。

![](https://ws4.sinaimg.cn/large/006tKfTcly1g0z96derqnj313c0b8t9p.jpg)